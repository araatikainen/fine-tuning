{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical assignment week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical Assignment: Sentiment Analysis System with\n",
    "Custom Fine-Tuned Model and Llama 3 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Dataset Preparation and Fine-Tuning (7 points)\n",
    "#### Step 1: Download the IMDB Dataset (1 point)\n",
    "1. Use the IMDB dataset from Kaggle: /kaggle/input/imdb-dataset/IMDB\n",
    "Dataset.csv.\n",
    "2. Load the dataset using Pandas and verify it in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load IMDB dataset from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
      "  inflating: IMDB Dataset.csv        \n"
     ]
    }
   ],
   "source": [
    "#!kaggle datasets list -s IMDB\n",
    "#!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "#!unzip imdb-dataset-of-50k-movie-reviews.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "imdb_dataset = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "print(imdb_dataset.head())\n",
    "imdb_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Data Preprocessing (1 point)\n",
    "1. Clean and preprocess the dataset:\n",
    "- Encode the sentiment column (positive -> 1, negative -> 0).\n",
    "- Retain only the review and label columns.\n",
    "2. Split the data into training and validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production  The filming tec...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there s a family where a little boy ...          0\n",
      "4  Petter Mattei s  Love in the Time of Money  is...          1\n"
     ]
    }
   ],
   "source": [
    "# clean the text data\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "imdb_dataset[\"review\"] = imdb_dataset[\"review\"].apply(clean_text)\n",
    "\n",
    "# remove long sentences (more than 400 words)\n",
    "#max_length = 400\n",
    "#imdb_dataset[\"review_length\"] = imdb_dataset[\"review\"].apply(lambda x: len(x.split()))\n",
    "#imdb_dataset = imdb_dataset[imdb_dataset[\"review_length\"] <= max_length]\n",
    "\n",
    "imdb_dataset[\"sentiment\"] = imdb_dataset[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(imdb_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training, validation and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = imdb_dataset[\"review\"]\n",
    "y = imdb_dataset[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Model Selection and Tokenization (1 point)\n",
    "1. Select a pre-trained Hugging Face transformer model for fine-tuning (e.g.,\n",
    "distilbert-base-uncased).\n",
    "2. Tokenize the dataset with (see if required )\n",
    "- Truncation.\n",
    "- Padding.\n",
    "- Maximum sequence length of 256.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DistilBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked  They are right  as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence  which set in right from the word GO  Trust me  this is not a show for the faint hearted or timid  This show pulls no punches with regards to drugs  sex or violence  Its is hardcore  in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary  It focuses mainly on Emerald City  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  Em City is home to many  Aryans  Muslims  gangstas  Latinos  Christians  Italians  Irish and more    so scuffles  death stares  dodgy dealings and shady agreements are never far away I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  Forget pretty pictures painted for mainstream audiences  forget charm  forget romance   OZ doesn t mess around  The first episode I ever saw struck me as so nasty it was surreal  I couldn t say I was ready for it  but as I watched more  I developed a taste for Oz  and got accustomed to the high levels of graphic violence  Not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  Watching Oz  you may become comfortable with what is uncomfortable viewing    thats if you can get in touch with your darker side \n",
      "Tokenized Sentence:  {'input_ids': [101, 2028, 1997, 1996, 2060, 15814, 2038, 3855, 2008, 2044, 3666, 2074, 1015, 11472, 2792, 2017, 2222, 2022, 13322, 2027, 2024, 2157, 2004, 2023, 2003, 3599, 2054, 3047, 2007, 2033, 1996, 2034, 2518, 2008, 4930, 2033, 2055, 11472, 2001, 2049, 24083, 1998, 4895, 10258, 2378, 8450, 5019, 1997, 4808, 2029, 2275, 1999, 2157, 2013, 1996, 2773, 2175, 3404, 2033, 2023, 2003, 2025, 1037, 2265, 2005, 1996, 8143, 18627, 2030, 5199, 3593, 2023, 2265, 8005, 2053, 17957, 2007, 12362, 2000, 5850, 3348, 2030, 4808, 2049, 2003, 13076, 1999, 1996, 4438, 2224, 1997, 1996, 2773, 2009, 2003, 2170, 11472, 2004, 2008, 2003, 1996, 8367, 2445, 2000, 1996, 17411, 4555, 3036, 2110, 7279, 4221, 12380, 2854, 2009, 7679, 3701, 2006, 14110, 2103, 2019, 6388, 2930, 1997, 1996, 3827, 2073, 2035, 1996, 4442, 2031, 3221, 21430, 1998, 2227, 20546, 2015, 2061, 9394, 2003, 2025, 2152, 2006, 1996, 11376, 7861, 2103, 2003, 2188, 2000, 2116, 26030, 2015, 7486, 18542, 10230, 7402, 2015, 8135, 16773, 3493, 1998, 2062, 2061, 8040, 16093, 28331, 2331, 14020, 26489, 6292, 24069, 1998, 22824, 10540, 2024, 2196, 2521, 2185, 1045, 2052, 2360, 1996, 2364, 5574, 1997, 1996, 2265, 2003, 2349, 2000, 1996, 2755, 2008, 2009, 3632, 2073, 2060, 3065, 2876, 1056, 8108, 5293, 3492, 4620, 4993, 2005, 7731, 9501, 5293, 11084, 5293, 7472, 11472, 2987, 1056, 6752, 2105, 1996, 2034, 2792, 1045, 2412, 2387, 4930, 2033, 2004, 2061, 11808, 2009, 2001, 16524, 1045, 2481, 1056, 2360, 1045, 2001, 3201, 2005, 2009, 2021, 2004, 1045, 3427, 2062, 1045, 2764, 1037, 5510, 2005, 11472, 1998, 2288, 17730, 2000, 1996, 2152, 3798, 1997, 8425, 4808, 2025, 2074, 4808, 2021, 21321, 15274, 4932, 2040, 2222, 2022, 2853, 2041, 2005, 1037, 15519, 13187, 2040, 2222, 3102, 2006, 2344, 1998, 2131, 2185, 2007, 2009, 2092, 5450, 2098, 2690, 2465, 13187, 2108, 2357, 2046, 3827, 7743, 2229, 2349, 2000, 2037, 3768, 1997, 2395, 4813, 2030, 3827, 3325, 3666, 11472, 2017, 2089, 2468, 6625, 2007, 2054, 2003, 8796, 10523, 2008, 2015, 2065, 2017, 2064, 2131, 1999, 3543, 2007, 2115, 9904, 2217, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize imdb dataset\n",
    "print(\"Original Sentence: \", imdb_dataset[\"review\"][0])\n",
    "print(\"Tokenized Sentence: \", tokenizer(imdb_dataset[\"review\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243c2c4d5f724c9f9e275ed1940c60c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec64569e301471f9ff9f68c6fc74b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4554e28741b745e89f5eaadda725f1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Tokenize the training, validation and testing dataset\n",
    "train_data = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "test_data = Dataset.from_dict({\"text\": X_test, \"label\": y_test})\n",
    "val_data = Dataset.from_dict({\"text\": X_val, \"label\": y_val})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "val_data = val_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 32000\n",
      "})\n",
      "{'text': 'With no fault to the actors  they all put on great performances   the overall story was not very well executed  The movie opens with a great zinger  a crazy old guy forces a young Aborigine girl s car off the road  But then  we re forced to endure 40 minutes of character development with an entirely new group of characters     and we don t know why until the 40 minutes are up  It turns out that they are the ones who eventually discover the girl s body     and the story progresses from there While the story does pick up at that point  it really goes nowhere  After 2 hours  I asked myself  was there a point to this  or was it just to see the characters struggle with accusations of racism and stupidity of how they handled the discovery  The story was ultimately unsatisfying and felt unfinished  While it is well acted  there s not a strong enough backbone in the film to warrant recommending it ', 'label': 0, 'input_ids': [101, 2007, 2053, 6346, 2000, 1996, 5889, 2027, 2035, 2404, 2006, 2307, 4616, 1996, 3452, 2466, 2001, 2025, 2200, 2092, 6472, 1996, 3185, 7480, 2007, 1037, 2307, 1062, 9912, 1037, 4689, 2214, 3124, 2749, 1037, 2402, 11113, 10050, 11528, 2063, 2611, 1055, 2482, 2125, 1996, 2346, 2021, 2059, 2057, 2128, 3140, 2000, 18094, 2871, 2781, 1997, 2839, 2458, 2007, 2019, 4498, 2047, 2177, 1997, 3494, 1998, 2057, 2123, 1056, 2113, 2339, 2127, 1996, 2871, 2781, 2024, 2039, 2009, 4332, 2041, 2008, 2027, 2024, 1996, 3924, 2040, 2776, 7523, 1996, 2611, 1055, 2303, 1998, 1996, 2466, 22901, 2013, 2045, 2096, 1996, 2466, 2515, 4060, 2039, 2012, 2008, 2391, 2009, 2428, 3632, 7880, 2044, 1016, 2847, 1045, 2356, 2870, 2001, 2045, 1037, 2391, 2000, 2023, 2030, 2001, 2009, 2074, 2000, 2156, 1996, 3494, 5998, 2007, 13519, 1997, 14398, 1998, 28072, 1997, 2129, 2027, 8971, 1996, 5456, 1996, 2466, 2001, 4821, 4895, 16846, 2483, 14116, 1998, 2371, 14342, 2096, 2009, 2003, 2092, 6051, 2045, 1055, 2025, 1037, 2844, 2438, 21505, 1999, 1996, 2143, 2000, 10943, 16755, 2075, 2009, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Display the tokenized dataset\n",
    "print(train_data)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Fine-Tune the Model (2 points)\n",
    "1. Fine-tune the model on the IMDB dataset for 2 epochs using the Hugging Face\n",
    "Trainer.\n",
    "2. Set training parameters:\n",
    "- Learning rate: 5e-5 or your own\n",
    "- Batch size: 16 or 32\n",
    "- Evaluation at the end of each epoch.\n",
    "3. Ensure that metrics like accuracy, precision, recall, and F1-score are logged\n",
    "during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and set training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the distilBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    './results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs = 2,\n",
    "    logging_steps = 200,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 32,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure metrics, accuracy, precision, recall and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def custom_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1) \n",
    "    \n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Trainer and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 08:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.226436</td>\n",
       "      <td>0.910125</td>\n",
       "      <td>0.882191</td>\n",
       "      <td>0.948775</td>\n",
       "      <td>0.914272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>0.221339</td>\n",
       "      <td>0.918750</td>\n",
       "      <td>0.920198</td>\n",
       "      <td>0.918832</td>\n",
       "      <td>0.919515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.2092179012298584, metrics={'train_runtime': 507.8392, 'train_samples_per_second': 126.024, 'train_steps_per_second': 1.969, 'total_flos': 4238956756992000.0, 'train_loss': 0.2092179012298584, 'epoch': 2.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=custom_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.23785458505153656, 'eval_accuracy': 0.916375, 'eval_precision': 0.91162109375, 'eval_recall': 0.9240287057658996, 'eval_f1': 0.9177829666953423, 'eval_runtime': 22.5144, 'eval_samples_per_second': 355.328, 'eval_steps_per_second': 5.552, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Save and Upload the Model to Hugging Face (2 points)\n",
    "1. Save the fine-tuned model and tokenizer locally using save_pretrained().\n",
    "2. Log in to Hugging Face using notebook_login.\n",
    "3. Upload the model to Hugging Face using push_to_hub.\n",
    "4. Verify the model on Hugging Face Hub and include the link in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./distilbert_imdb_sentiment_analysis_tokenizer/tokenizer_config.json',\n",
       " './distilbert_imdb_sentiment_analysis_tokenizer/special_tokens_map.json',\n",
       " './distilbert_imdb_sentiment_analysis_tokenizer/vocab.txt',\n",
       " './distilbert_imdb_sentiment_analysis_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model using save_pretrained\n",
    "model_path = \"./distilbert_imdb_sentiment_analysis\"\n",
    "tokenizer_path = \"./distilbert_imdb_sentiment_analysis_tokenizer\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (0.26.2)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raati/anaconda3/envs/gptenv/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.26.2\n",
      "    Uninstalling huggingface-hub-0.26.2:\n",
      "      Successfully uninstalled huggingface-hub-0.26.2\n",
      "Successfully installed huggingface_hub-0.28.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622bd671a2394e11a76ae32655b9e903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909c1b724c334bd2ab0b905fe234ae1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/raati/distilbert_imdb_sentiment_analysis_tokenizer/commit/754b8a5f3ad48ce47a7bb312dcdf53ec358ee848', commit_message='Upload tokenizer', commit_description='', oid='754b8a5f3ad48ce47a7bb312dcdf53ec358ee848', pr_url=None, repo_url=RepoUrl('https://huggingface.co/raati/distilbert_imdb_sentiment_analysis_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='raati/distilbert_imdb_sentiment_analysis_tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the model and tokenizer\n",
    "model.push_to_hub(\"raati/distilbert_imdb_sentiment_analysis\")\n",
    "tokenizer.push_to_hub(\"raati/distilbert_imdb_sentiment_analysis_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and tokenizer has been successfully uploaded to Hugging Face. They can be found from\n",
    "\n",
    "model: https://huggingface.co/raati/distilbert_imdb_sentiment_analysis\n",
    "\n",
    "tokenizer: https://huggingface.co/raati/distilbert_imdb_sentiment_analysis_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading hugging face model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e042453fc8604a279484c96115338d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467b6754efef4d2398298202b6122247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d1d20549b44a46bbd7434f1bfe9b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9886391162872314}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"raati/distilbert_imdb_sentiment_analysis\", tokenizer=\"raati/distilbert_imdb_sentiment_analysis_tokenizer\")\n",
    "print(classifier(\"This movie was fantastic!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model seems to classify sentence correctly as positive (label 1 = positive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
